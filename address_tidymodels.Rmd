---
title: "Address_tidymodels"
author: "KIM"
date: "2019/3/3"
output: html_document
---

# 载入库

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(jiebaR)
library(Rwordseg)
library(readxl)
library(tidytext)
library(textrecipes)
library(tidymodels)
library(doParallel)
library(tictoc)
```

# 原始数据读取

```{r}
mingxi <- read_xlsx("mingxi.xlsx")
mingxi
```

# 数据初步清理

```{r}
remove_str <- "[(待删除)|(仅限续费)|(不可报装)|(未验收)|(已光改)|(地址)|(指定渠道下单)|(管道商协议问题)|(物业问题)|(光纤改造中，暂不可下单)]"
mingxi_s <- 
mingxi %>% 
  filter(`属性`!= "校园") %>% 
  select(two = `二级地址(0116)`, three = `三级地址(0116)`, four = `四级地址(0116)`, five = `五级地址(0116)`, class = `属性`) %>% 
  unite(address, two, three, four, five, sep = "") %>% 
  mutate(id = row_number(),
         class = factor(class),
         address = str_remove_all(address, "[\\d|\\s|A-Za-z|\\p{P}]")) %>% 
  mutate(address = str_remove_all(address, remove_str))
mingxi_s
```

# 字典处理

在不能使用cidian包的情况下，借助Rwordseg包的installDict函数，将搜狗细胞库转化为txt（dic）格式的词典，将词典放入jieba的worker分词引擎中使用。
步骤：
1.在本工作空间中先建立文件夹custom_dict（或其他名称）
2.将dicmeta, option.rds和user.dic三个文件都放到custom_dict/Rwordseg目录下才能安装成功，user.dic中最少要有1行
3.使用Rwordseg包的installDict函数字典安装搜狗scel文件，安装后会自动累加写入user.dic文件（如果使用cidian包的decode函数，则每个词典会变为单独的txt）
4.将user.dic文件放入jieba包的worker()中进行调用。

安裝cidian包的使用方法：https://zhuanlan.zhihu.com/p/473619960
使用cidian包的情况下，将字典通过decode_scel函数解压成单个txt格式词典，可以人工合并在一起，放入jieba的worker分词引擎中使用。

```{r}
# tempdir() 
# 设置字典文件，要先建立文件夹
setAppDir("custom_dict") 
dict <- dir("./Dict", pattern = ".scel$") 
# 循环安装字典，会累加到user.dic中，user.dic原始文件中至少要有1条数据
for (i in 1:length(dict)){
  Rwordseg::installDict(file.path("./Dict", dict[i]))
  print(sprintf("已完成第%d个词典", i))
 }   
listDict()
```

# 使用tidymodels进行建模

```{r}
# 划分数据集
set.seed(100)
mingxi_s_split <- initial_split(mingxi_s, prop = 0.80, strata = class)
mingxi_s_train <- training(mingxi_s_split)
mingxi_s_test  <-  testing(mingxi_s_split)
mingxi_s_train
mingxi_s_test
```

# jieba分词

```{r}
# 设定分词引擎
jieba_tokenizer <- function(x) {
  jieba_worker = worker(
    bylines = TRUE, 
    stop_word = "./custom_dict/Rwordseg/stopwordsCN.txt", 
    dict = "./custom_dict/Rwordseg/user.dic"
    ) # 结果为列表
  segment(x, jiebar = jieba_worker)
}
```


```{r}
# 使用自定义分词引擎
rec_token_jb <- 
  recipe(class ~ ., data = mingxi_s_train) %>%
  update_role(id, new_role = "ID") %>% 
  step_tokenize(address, custom_token = jieba_tokenizer) %>% 
  step_tokenfilter(address, min_times = 2) %>% 
  step_tfidf(address) %>% 
  step_normalize(all_numeric_predictors())
tidy(rec_token_jb)
rec_token_jb %>% prep() %>% bake(new_data = NULL)
# rec_token_jb %>% prep() %>% bake(new_data = mingxi_s_test)
```

```{r}
# 建立空模型
model_null <- 
  null_model() %>% 
  set_engine("parsnip") %>% 
  set_mode("classification")
model_null
```

```{r}
# 建立模型
model_rf <- 
  rand_forest(trees = 100, mtry = 3) %>%
  set_engine("ranger", seed = 100, num.threads = 12, verbose = TRUE) %>%
  set_mode("classification")
model_rf
```

```{r}
# 建立工作流
wf_null <- 
  workflow() %>%
  add_recipe(rec_token_jb) %>%
  add_model(model_null)

wf_rf <- 
  workflow() %>%
  add_recipe(rec_token_jb) %>%
  add_model(model_rf)
```


```{r}
# 模型拟合
wf_null_fit <- wf_null %>% fit(mingxi_s_train)

# 查看结果
wf_null_fit %>% extract_fit_parsnip()

# 预测测试集
test_pred_truth <- 
  bind_cols(
    wf_null_fit %>% predict(mingxi_s_test) %>% select(.pred_class),
    mingxi_s_test %>% select(class)
) %>%
  rename(truth = class,
         pred = .pred_class)

# 查看结果
accuracy(data = test_pred_truth, truth = truth, estimate = pred)
conf_mat(data = test_pred_truth, truth = truth, estimate = pred)
```

```{r}
# 模型拟合
wf_rf_fit <- wf_rf %>% fit(mingxi_s_train)

# 查看结果
wf_rf_fit %>% extract_fit_parsnip()

# 预测测试集
test_pred_truth <- 
  bind_cols(
    wf_rf_fit %>% predict(mingxi_s_test) %>% select(.pred_class),
    mingxi_s_test %>% select(class)
) %>%
  rename(truth = class,
         pred = .pred_class)

# 查看结果
accuracy(data = test_pred_truth, truth = truth, estimate = pred)
conf_mat(data = test_pred_truth, truth = truth, estimate = pred)
```

```{r}
# 交叉检验设置
set.seed(200)
cv_5 <- vfold_cv(mingxi_s_train, v = 5)
metric_acc <- metric_set(accuracy)
```


```{r}
# 设定并行集群
cl <- makeCluster(10)
registerDoParallel(cl)
tic()
wf_rf_cv_5 <- 
  wf_rf %>% 
  fit_resamples(
    resamples = cv_5, 
    metrics = metric_acc, 
    control = control_resamples(parallel_over = NULL)
    )
toc()
stopCluster(cl)
```

```{r}
wf_rf_cv_3
wf_rf_cv_3 %>% collect_metrics()
```

