---
title: "Address_tidymodels"
author: "KIM"
date: "2019/3/3"
output: html_document
---

# 载入库

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(jiebaR)
library(Rwordseg)
library(readxl)
library(tidytext)
library(textrecipes)
library(tidymodels)
library(doParallel)
library(tictoc)
library(discrim)
```

# 原始数据读取

```{r}
mingxi <- read_xlsx("mingxi.xlsx")
mingxi
```

# 数据初步清理

```{r}
remove_str <- "[(待删除)|(仅限续费)|(不可报装)|(未验收)|(已光改)|(地址)|(指定渠道下单)|(管道商协议问题)|(物业问题)|(光纤改造中，暂不可下单)]"
mingxi_s <- 
mingxi %>% 
  filter(`属性`!= "校园") %>% 
  select(two = `二级地址(0116)`, three = `三级地址(0116)`, four = `四级地址(0116)`, five = `五级地址(0116)`, class = `属性`) %>% 
  unite(address, two, three, four, five, sep = "") %>% 
  mutate(id = row_number(),
         class = factor(class),
         address = str_remove_all(address, "[\\d|\\s|A-Za-z|\\p{P}]")) %>% 
  mutate(address = str_remove_all(address, remove_str))
mingxi_s
```

# 字典处理

在不能使用cidian包的情况下，借助Rwordseg包的installDict函数，将搜狗细胞库转化为txt（dic）格式的词典，将词典放入jieba的worker分词引擎中使用。
步骤：
1.在本工作空间中先建立文件夹custom_dict（或其他名称）
2.将dicmeta, option.rds和user.dic三个文件都放到custom_dict/Rwordseg目录下才能安装成功，user.dic中最少要有1行
3.使用Rwordseg包的installDict函数字典安装搜狗scel文件，安装后会自动累加写入user.dic文件（如果使用cidian包的decode函数，则每个词典会变为单独的txt）
4.将user.dic文件放入jieba包的worker()中进行调用。

安裝cidian包的使用方法：https://zhuanlan.zhihu.com/p/473619960
使用cidian包的情况下，将字典通过decode_scel函数解压成单个txt格式词典，可以人工合并在一起，放入jieba的worker分词引擎中使用。

```{r}
# tempdir() 
# 设置字典文件，要先建立文件夹
setAppDir("custom_dict") 
dict <- dir("./Dict", pattern = ".scel$") 
# 循环安装字典，会累加到user.dic中，user.dic原始文件中至少要有1条数据
for (i in 1:length(dict)){
  Rwordseg::installDict(file.path("./Dict", dict[i]))
  print(sprintf("已完成第%d个词典", i))
 }   
listDict()
```

# 使用tidymodels进行建模

```{r}
# 划分数据集
set.seed(200)
mingxi_s_split <- initial_split(mingxi_s, prop = 0.80, strata = class)
mingxi_s_train <- training(mingxi_s_split)
mingxi_s_test  <- testing(mingxi_s_split)
mingxi_s_train
mingxi_s_test
```

# jieba分词

```{r}
# 设定分词引擎
jieba_tokenizer <- function(x) {
  jieba_worker = worker(
    bylines = TRUE, 
    stop_word = "./custom_dict/Rwordseg/stopwordsCN.txt", 
    dict = "./custom_dict/Rwordseg/user.dic"
    ) # 结果为列表
  segment(x, jiebar = jieba_worker)
}
```

# 建立预处理流程

```{r}
# 使用自定义分词引擎
rec_token_jb_tfidf <- 
  recipe(class ~ ., data = mingxi_s_train) %>%
  update_role(id, new_role = "ID") %>% 
  step_tokenize(address, custom_token = jieba_tokenizer) %>% 
  step_tokenfilter(address, max_tokens = 100) %>% 
  step_tfidf(address) %>% 
  step_normalize(all_numeric_predictors())
rec_token_jb_tfidf %>% prep() %>% bake(new_data = NULL)
rec_token_jb_tfidf %>% prep() %>% bake(new_data = mingxi_s_test)

# 哑变量
rec_token_jb_dummy <-
  recipe(class ~ ., data = mingxi_s_train) %>%
  update_role(id, new_role = "ID") %>%
  step_tokenize(address, custom_token = jieba_tokenizer) %>%
  step_tokenfilter(address, max_tokens = 100) %>%
  step_tf(address, weight_scheme = "binary")
rec_token_jb_dummy %>% prep() %>% bake(new_data = NULL)
rec_token_jb_dummy %>% prep() %>% bake(new_data = mingxi_s_test)
```

```{r}
# 建立空模型
model_null <- 
  null_model() %>% 
  set_engine("parsnip") %>% 
  set_mode("classification")
model_null

# 建立随机森林模型
model_rf <- 
  rand_forest(trees = 100, mtry = 3) %>%
  set_engine("ranger", seed = 100, num.threads = 12, verbose = TRUE) %>%
  set_mode("classification")
model_rf

# 贝叶斯模型
model_nb <- naive_Bayes(Laplace = 0.1) %>%
  set_mode("classification") %>%
  set_engine("naivebayes")
model_nb
```

```{r}
# 建立工作流
wf_null <- 
  workflow() %>%
  add_recipe(rec_token_jb_tfidf) %>%
  add_model(model_null)

wf_rf <- 
  workflow() %>%
  add_recipe(rec_token_jb_tfidf) %>%
  add_model(model_rf)

wf_nb <- 
  workflow() %>%
  add_recipe(rec_token_jb_dummy) %>%
  add_model(model_nb)
```


```{r}
# 空模型效果
wf_null_fit <- wf_null %>% fit(mingxi_s_train)
test_pred_truth <- 
  bind_cols(
    wf_null_fit %>% predict(mingxi_s_test) %>% select(.pred_class),
    mingxi_s_test %>% select(class)
) %>%
  rename(truth = class,
         pred = .pred_class)
accuracy(data = test_pred_truth, truth = truth, estimate = pred)
conf_mat(data = test_pred_truth, truth = truth, estimate = pred)
```

```{r}
# 随机森林效果
wf_rf_fit <- wf_rf %>% fit(mingxi_s_train)
test_pred_truth <- 
  bind_cols(
    wf_rf_fit %>% predict(mingxi_s_test) %>% select(.pred_class),
    mingxi_s_test %>% select(class)
) %>%
  rename(truth = class,
         pred = .pred_class)
accuracy(data = test_pred_truth, truth = truth, estimate = pred)
conf_mat(data = test_pred_truth, truth = truth, estimate = pred)
```

```{r}
# 贝叶斯效果
wf_nb_fit <- wf_nb %>% fit(mingxi_s_train)
test_pred_truth <- 
  bind_cols(
    wf_nb_fit %>% predict(mingxi_s_test) %>% select(.pred_class),
    mingxi_s_test %>% select(class)
) %>%
  rename(truth = class,
         pred = .pred_class)
accuracy(data = test_pred_truth, truth = truth, estimate = pred)
conf_mat(data = test_pred_truth, truth = truth, estimate = pred)
```

# K折交叉验证

```{r}
# 交叉检验设置
set.seed(200)
cv_5 <- vfold_cv(mingxi_s_train, v = 5, strata = class)
metric_acc <- metric_set(accuracy)

# 并行计算会报错
# cl <- makeCluster(10)
# registerDoParallel(cl)
tic()
wf_rf_cv_5 <- 
  wf_rf %>% 
  fit_resamples(
    resamples = cv_5, 
    metrics = metric_acc,
    control = control_resamples(parallel_over = "everything")
    )
toc()
# stopCluster(cl)
wf_rf_cv_5 %>% unnest(.notes)
wf_rf_cv_5 %>% collect_metrics()
```

# textrecipes

```{r}
library(recipes)
library(modeldata)
data(Smithsonian)
smith_tr <- Smithsonian[1:15, ]
smith_te <- Smithsonian[16:20, ]
rec <- recipe(~., data = smith_tr)
rec <- rec %>%
step_clean_levels(name)
rec <- prep(rec, training = smith_tr)
cleaned <- bake(rec, smith_tr)
tidy(rec, number = 1)
```

```{r}
library(recipes)
data(airquality)
air_tr <- tibble(airquality[1:100, ])
air_te <- tibble(airquality[101:153, ])
rec <- recipe(~., data = air_tr)
rec <- rec %>%
step_clean_names(all_predictors())
rec <- prep(rec, training = air_tr)
tidy(rec, number = 1)
bake(rec, air_tr)
bake(rec, air_te)
```

```{r}
library(recipes)
library(modeldata)
data(grants)
grants_rec <- recipe(~sponsor_code, data = grants_other) %>% step_dummy_hash(sponsor_code)
grants_obj <- grants_rec %>% prep()
bake(grants_obj, grants_test)
tidy(grants_rec, number = 1)
tidy(grants_obj, number = 1)
```

```{r}
library(recipes)
library(modeldata)
data(tate_text)
tate_rec <- recipe(~., data = tate_text) %>% 
  step_tokenize(medium) %>%
  step_lda(medium)
tate_obj <- tate_rec %>% prep()
bake(tate_obj, new_data = NULL) %>% slice(1:2)
tidy(tate_rec, number = 2)
tidy(tate_obj, number = 2)
# Changing the number of topics.
recipe(~., data = tate_text) %>%
step_tokenize(medium, artist) %>%
step_lda(medium, artist, num_topics = 20) %>% prep() %>% bake(new_data = NULL) %>% slice(1:2)
```

